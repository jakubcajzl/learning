{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Apache Spark Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /mnt/disk-1/anaconda3/lib/python3.7/site-packages (3.4.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /mnt/disk-1/anaconda3/lib/python3.7/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.0.3.3.7180.0-274\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 1.8.0_382\n",
      "Branch HEAD\n",
      "Compiled by user  on 2022-08-30T12:57:40Z\n",
      "Revision 73b4169950a4b83435306674db9e31e28529e8b5\n",
      "Url git@github.infra.cloudera.com:CDH/spark3.git\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "# Checking PySpark version:\n",
    "\n",
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Java (for JVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: / \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/noarch::flask-jwt-extended==3.24.1=py_0\n",
      "  - defaults/linux-64::anaconda==custom=py37_1\n",
      "  - defaults/noarch::flask-babel==0.12.2=py_1\n",
      "  - defaults/linux-64::flask-cors==3.0.7=py37_0\n",
      "  - defaults/noarch::flask-admin==1.5.4=py_0\n",
      "  - defaults/noarch::flask-sqlalchemy==2.4.4=py_0\n",
      "  - defaults/linux-64::flask-login==0.4.1=py37_0\n",
      "  - defaults/linux-64::flask-openid==1.2.5=py37_1003\n",
      "  - defaults/linux-64::airflow==1.10.10=py37_0\n",
      "  - defaults/noarch::flask-appbuilder==2.2.1=py_1\n",
      "  - defaults/noarch::flask-swagger==0.2.13=py_2\n",
      "  - defaults/linux-64::blaze==0.11.3=py37_0\n",
      "  - defaults/noarch::flask-caching==1.9.0=py_0\n",
      "  - defaults/noarch::flask==1.1.1=py_0\n",
      "  - defaults/noarch::flask-wtf==0.14.3=py_0\n",
      "  - defaults/linux-64::_anaconda_depends==2019.10=py37_0\n",
      "done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /mnt/disk-1/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - java-jdk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2024.3.11  |       h06a4308_0         134 KB\n",
      "    certifi-2022.12.7          |   py37h06a4308_0         152 KB\n",
      "    conda-4.12.0               |   py37h06a4308_0        16.9 MB\n",
      "    java-jdk-8.45.14           |                0       153.6 MB  cyclus\n",
      "    openssl-1.1.1w             |       h7f8727e_0         3.8 MB\n",
      "    werkzeug-1.0.1             |     pyhd3eb1b0_0         239 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       174.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  java-jdk           cyclus/linux-64::java-jdk-8.45.14-0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                     2021.10.26-h06a4308_2 --> 2024.3.11-h06a4308_0\n",
      "  certifi                          2021.10.8-py37h06a4308_0 --> 2022.12.7-py37h06a4308_0\n",
      "  conda                               4.10.3-py37h06a4308_0 --> 4.12.0-py37h06a4308_0\n",
      "  openssl                                 1.1.1l-h7f8727e_0 --> 1.1.1w-h7f8727e_0\n",
      "  werkzeug           pkgs/main/linux-64::werkzeug-0.14.1-p~ --> pkgs/main/noarch::werkzeug-1.0.1-pyhd3eb1b0_0\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? "
     ]
    }
   ],
   "source": [
    "!conda install -c cyclus java-jdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kerberos login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception: too many parameters\n",
      "java.lang.IllegalArgumentException: too many parameters\n",
      "\tat sun.security.krb5.internal.tools.KinitOptions.<init>(KinitOptions.java:153)\n",
      "\tat sun.security.krb5.internal.tools.Kinit.<init>(Kinit.java:147)\n",
      "\tat sun.security.krb5.internal.tools.Kinit.main(Kinit.java:113)\n"
     ]
    }
   ],
   "source": [
    "# Closing all active Kerberos connections (tickets):\n",
    "#!kdestroy\n",
    "\n",
    "# Starting a Kerberos connection:\n",
    "!kinit -kt login/Jakub.Cajzl.keytab Jakub.Cajzl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials cache C:\\Users\\Jakub.Cajzl\\krb5cc_Jakub.Cajzl not found.\n"
     ]
    }
   ],
   "source": [
    "# Displaying active tickets:\n",
    "!klist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries:\n",
    "import sys, os\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext, StorageLevel\n",
    "from pyspark.sql import SparkSession, DataFrame, SQLContext\n",
    "# from pyspark.sql import types as T, functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "\n",
    "# It is better to load pySpark version of Pandas as 'ps' instead of 'pd'\n",
    "# because pySpark has differences in their version of Pandas \n",
    "# (mainly because pySpark is masivelly parallelized and has many \n",
    "# parallelization parameters).\n",
    "# from pyspark import pandas as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName(\"app_01\")\n",
    "            .getOrCreate()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other option:\n",
    "#sc = SparkContext(\"local\")\n",
    "#spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code - Tasks\n",
    "\n",
    "In the following sub-chapters and lines of code are tasks aimed to get familiar with Apache Spark DataFrame API and SQL API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Create a DataFrame using createDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Defining schema:\n",
    "my_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"surname\", StringType(), True),\n",
    "    StructField(\"sex\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"height\", FloatType(), True)\n",
    "    ])\n",
    "\n",
    "# 50 most common US male and female names:\n",
    "male_names = [\"James\", \"John\", \"Robert\", \"Michael\", \"William\", \"David\", \"Richard\", \"Joseph\", \"Thomas\", \"Charles\", \n",
    "              \"Christopher\", \"Daniel\", \"Matthew\", \"Anthony\", \"Mark\", \"Donald\", \"Steven\", \"Paul\", \"Andrew\", \"Joshua\", \n",
    "              \"Kenneth\", \"Kevin\", \"Brian\", \"George\", \"Edward\", \"Ronald\", \"Timothy\", \"Jason\", \"Jeffrey\", \"Ryan\", \n",
    "              \"Jacob\", \"Gary\", \"Nicholas\", \"Eric\", \"Jonathan\", \"Stephen\", \"Larry\", \"Justin\", \"Scott\", \"Brandon\", \n",
    "              \"Benjamin\", \"Samuel\", \"Gregory\", \"Frank\", \"Alexander\", \"Raymond\", \"Patrick\", \"Jack\", \"Dennis\", \"Jerry\"]\n",
    "              \n",
    "female_names = [\"Mary\", \"Patricia\", \"Jennifer\", \"Linda\", \"Elizabeth\", \"Barbara\", \"Susan\", \"Jessica\", \"Sarah\", \"Karen\", \n",
    "                \"Nancy\", \"Lisa\", \"Margaret\", \"Betty\", \"Sandra\", \"Ashley\", \"Kimberly\", \"Emily\", \"Donna\", \"Michelle\", \n",
    "                \"Dorothy\", \"Carol\", \"Amanda\", \"Melissa\", \"Deborah\", \"Stephanie\", \"Rebecca\", \"Sharon\", \"Laura\", \"Cynthia\", \n",
    "                \"Kathleen\", \"Amy\", \"Shirley\", \"Angela\", \"Helen\", \"Anna\", \"Brenda\", \"Pamela\", \"Nicole\", \"Samantha\", \n",
    "                \"Katherine\", \"Emma\", \"Ruth\", \"Christine\", \"Catherine\", \"Debra\", \"Rachel\", \"Carolyn\", \"Janet\", \"Virginia\"]\n",
    "\n",
    "# 100 most common US surnames:\n",
    "surnames = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Garcia\", \"Miller\", \"Davis\", \"Rodriguez\", \"Martinez\",\n",
    "            \"Hernandez\", \"Lopez\", \"Gonzalez\", \"Wilson\", \"Anderson\", \"Thomas\", \"Taylor\", \"Moore\", \"Jackson\", \"Martin\",\n",
    "            \"Lee\", \"Perez\", \"Thompson\", \"White\", \"Harris\", \"Sanchez\", \"Clark\", \"Ramirez\", \"Lewis\", \"Robinson\",\n",
    "            \"Walker\", \"Young\", \"Allen\", \"King\", \"Wright\", \"Scott\", \"Torres\", \"Nguyen\", \"Hill\", \"Flores\",\n",
    "            \"Green\", \"Adams\", \"Nelson\", \"Baker\", \"Hall\", \"Rivera\", \"Campbell\", \"Mitchell\", \"Carter\", \"Roberts\",\n",
    "            \"Gomez\", \"Phillips\", \"Evans\", \"Turner\", \"Diaz\", \"Parker\", \"Cruz\", \"Edwards\", \"Collins\", \"Reyes\",\n",
    "            \"Stewart\", \"Morris\", \"Morales\", \"Murphy\", \"Cook\", \"Rogers\", \"Gutierrez\", \"Ortiz\", \"Morgan\", \"Cooper\",\n",
    "            \"Peterson\", \"Bailey\", \"Reed\", \"Kelly\", \"Howard\", \"Ramos\", \"Kim\", \"Cox\", \"Ward\", \"Richardson\",\n",
    "            \"Watson\", \"Brooks\", \"Chavez\", \"Wood\", \"James\", \"Bennett\", \"Gray\", \"Mendoza\", \"Ruiz\", \"Hughes\",\n",
    "            \"Price\", \"Alvarez\", \"Castillo\", \"Sanders\", \"Patel\", \"Myers\", \"Long\", \"Ross\", \"Foster\", \"Jimenez\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(1, 101):\n",
    "    \n",
    "    id = i\n",
    "    \n",
    "    sex = random.choice(['Male','Female'])\n",
    "    \n",
    "    # Creating random 50 male and 50 female names:\n",
    "    if sex == 'Male':\n",
    "        name = random.choice(male_names)\n",
    "    elif sex == 'Female':\n",
    "        name = random.choice(female_names)\n",
    "        \n",
    "    surname = random.choice(surnames)\n",
    "        \n",
    "    age = random.randint(18, 70)\n",
    "        \n",
    "    location = random.choice([\n",
    "        'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',\n",
    "        'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa',\n",
    "        'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan',\n",
    "        'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n",
    "        'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma',\n",
    "        'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee',\n",
    "        'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming'\n",
    "        ])\n",
    "        \n",
    "    height = round(random.uniform(1.5, 2.0), 2)\n",
    "    \n",
    "    data.append((id, name, surname, sex, age, location, height))\n",
    "    \n",
    "# Creating DataFrame:\n",
    "df = spark.createDataFrame(data, schema=my_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Display schema of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: int, name: string, surname: string, sex: string, age: int, location: string, height: float]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- height: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o38.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (Cloud-CQKD9G3.adastracorp.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jakub.Cajzl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Jakub.Cajzl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Jakub.Cajzl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Jakub.Cajzl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Jakub.Cajzl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o38.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (Cloud-CQKD9G3.adastracorp.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Display the DataFrame and row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+------------+------+\n",
      "| id|    name|   surname|   sex|age|    location|height|\n",
      "+---+--------+----------+------+---+------------+------+\n",
      "|  1|     Amy| Rodriguez|Female| 54|North Dakota|  1.65|\n",
      "|  2| Deborah|  Campbell|Female| 31|     Indiana|  1.51|\n",
      "|  3|   Debra|     White|Female| 45|       Maine|   1.5|\n",
      "|  4|Margaret|     Smith|Female| 69|    Missouri|  1.92|\n",
      "|  5|   Helen|    Morgan|Female| 22|     Vermont|  1.78|\n",
      "|  6| Charles|    Morris|  Male| 28|     Georgia|  1.69|\n",
      "|  7|   Emily|    Miller|Female| 28|South Dakota|  1.69|\n",
      "|  8|    Mark|    Parker|  Male| 61|Rhode Island|  1.84|\n",
      "|  9| Gregory|    Morris|  Male| 68|   Louisiana|   1.6|\n",
      "| 10|  Brenda|      Cook|Female| 51|    Illinois|  1.57|\n",
      "| 11|  Joseph|    Miller|  Male| 39|    Arkansas|  1.58|\n",
      "| 12|    Emma|    Wilson|Female| 41|    Michigan|  1.58|\n",
      "| 13|  Ashley|    Harris|Female| 63| Connecticut|  1.62|\n",
      "| 14|Virginia|     Ramos|Female| 30|  New Mexico|   1.5|\n",
      "| 15| Rebecca|      Hall|Female| 28|   Minnesota|  1.92|\n",
      "| 16| Charles|   Edwards|  Male| 22|    Illinois|  1.81|\n",
      "| 17|   Frank|    Torres|  Male| 44|        Iowa|  1.82|\n",
      "| 18|   Betty|    Chavez|Female| 66|    Arkansas|  1.66|\n",
      "| 19|  Joshua|Richardson|  Male| 35|     Indiana|  1.68|\n",
      "| 20|  Dennis|     White|  Male| 69|     Alabama|  1.63|\n",
      "+---+--------+----------+------+---+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+---+---+--------+------+\n",
      "| id|name|surname|sex|age|location|height|\n",
      "+---+----+-------+---+---+--------+------+\n",
      "|  1| Jac|    Alv|Mal| 25|     Wis|   1.9|\n",
      "|  2| Ben|    Tur|Mal| 52|     Sou|   1.6|\n",
      "|  3| Dav|    Pet|Mal| 46|     Con|   1.7|\n",
      "+---+----+-------+---+---+--------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing only 3 top rows and truncating output to 3 characters:\n",
    "\n",
    "df.show(n=3, truncate=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 100\n"
     ]
    }
   ],
   "source": [
    "# Displaying the row count:\n",
    "print(f\"Row count: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of males: 53\n",
      "Number of females: 47\n"
     ]
    }
   ],
   "source": [
    "# Counting males and females in the dataset:\n",
    "males_count = df.filter(col('sex')=='Male').count()\n",
    "females_count = df.filter(col('sex')=='Female').count()\n",
    "\n",
    "print(f\"Number of males: {males_count}\\nNumber of females: {females_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Save the created DataFrame in parquet and csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write\n",
    "    .format(\"parquet\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"created_dataset_parquet\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write\n",
    "    .format(\"csv\")\n",
    "    #.partitionBy(\"id\")\n",
    "    .option(\"header\", True)\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"created_dataset_csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created_dataset_csv\t final_dataset.csv   PySpark_Kaggle_Dataset.ipynb\n",
      "created_dataset_parquet  Jakub.Cajzl.keytab  Spark_Excercises.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Listing a main directory (should contain _csv and _parquet folders):\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Change a number of partitions and re-save the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions for 'df' DataFrame: 2\n"
     ]
    }
   ],
   "source": [
    "# Displaying a number of partitions:\n",
    "print(f\"Number of partitions for 'df' DataFrame: {df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartitioning to 1 partition:\n",
    "df = df.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions for 'df' DataFrame: 1\n"
     ]
    }
   ],
   "source": [
    "# Displaying the NEW number of partitions:\n",
    "print(f\"Number of partitions for 'df' DataFrame: {df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-saving a dataframe with only 1 partition:\n",
    "(df.write\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"created_dataset_csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-65697176-6d7a-4864-adeb-03fd9ba0f7ad-c000.csv  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Listing a CSV directory (should contain 1 CSV file):\n",
    "!ls created_dataset_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Rename saved partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for listing a directory:\n",
    "def list_directory(path: str):\n",
    "    '''\n",
    "    Function for listing files in the given directory omitting hidden files and \"_SUCCESS\" file. \n",
    "    '''\n",
    "    for item in os.listdir(path):\n",
    "        if not (item.startswith(\".\") or item == \"_SUCCESS\"):\n",
    "            return(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created_dataset_csv\t final_dataset.csv   PySpark_Kaggle_Dataset.ipynb\n",
      "created_dataset_parquet  Jakub.Cajzl.keytab  Spark_Excercises.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Checking the saved datasets:\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-65697176-6d7a-4864-adeb-03fd9ba0f7ad-c000.csv  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of partitions:\n",
    "!ls created_dataset_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the directory 'created_dataset_csv':\n",
      "dataset.csv\n"
     ]
    }
   ],
   "source": [
    "def rename_csv_partitions(directory: str, partition_name: str):\n",
    "    '''\n",
    "    Function for renaming saved default CSV partitions created by Spark.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory (string): a directory where CSV files are located\n",
    "    - partition_name (string): new name for CSV partition(s) without a .csv extension\n",
    "    '''\n",
    "    \n",
    "    new_name = f\"{directory}/{partition_name}.csv\"\n",
    "\n",
    "    # Renaming all the CSV files in the directory:\n",
    "    for item in os.listdir(directory):\n",
    "        if item.endswith(\".csv\"):\n",
    "            old_name = f\"{directory}/{item}\"\n",
    "            os.rename(old_name, new_name)\n",
    "\n",
    "\n",
    "# Renaming the partitions:\n",
    "directory = \"created_dataset_csv\"\n",
    "partition_name = \"dataset\"\n",
    "\n",
    "#rename_csv_partitions(directory, partition_name)\n",
    "\n",
    "# Listing the main directory with renamed .csv file(s):\n",
    "print(f\"Contents of the directory '{directory}':\\n{list_directory(directory)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Load a CSV file without and with header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------+------+---+-------------+------+\n",
      "|_c0|     _c1|      _c2|   _c3|_c4|          _c5|   _c6|\n",
      "+---+--------+---------+------+---+-------------+------+\n",
      "| id|    name|  surname|   sex|age|     location|height|\n",
      "|  1|   Debra|   Walker|Female| 22|      Alabama|  1.67|\n",
      "|  2|   Helen|  Jimenez|Female| 19| Pennsylvania|   1.6|\n",
      "|  3|  Andrew|   Miller|  Male| 66|     Nebraska|  1.79|\n",
      "|  4| Kenneth|   Thomas|  Male| 35|     New York|  1.68|\n",
      "|  5|    John|   Miller|  Male| 69|   New Mexico|  1.61|\n",
      "|  6|Benjamin|    Lewis|  Male| 61|     Michigan|  1.93|\n",
      "|  7|   Linda|    Baker|Female| 45|     New York|  1.74|\n",
      "|  8| Carolyn|    Lewis|Female| 31|      Vermont|  1.61|\n",
      "|  9|    Gary|     Wood|  Male| 70|      Wyoming|  1.83|\n",
      "| 10|Jennifer|   Harris|Female| 30|        Maine|  1.74|\n",
      "| 11| Michael|Gutierrez|  Male| 31|West Virginia|  1.53|\n",
      "| 12|   Linda|    Patel|Female| 44|      Arizona|  1.51|\n",
      "| 13| Jessica|  Jimenez|Female| 59| North Dakota|  1.78|\n",
      "| 14| Jeffrey|   Martin|  Male| 18|    Tennessee|  1.61|\n",
      "| 15| Barbara|    Ortiz|Female| 39|New Hampshire|  1.55|\n",
      "| 16|   Donna|    Reyes|Female| 39|  Mississippi|  1.56|\n",
      "| 17|   Kevin|    Green|  Male| 34|      Arizona|  1.52|\n",
      "| 18| Kenneth|      Lee|  Male| 45|     Maryland|  1.74|\n",
      "| 19|   Brian|  Mendoza|  Male| 37|   California|  1.82|\n",
      "+---+--------+---------+------+---+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading a CSV without a header:\n",
    "df2 = spark.read.csv(\"created_dataset_csv\")\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+------+----+-------------+------+\n",
      "| id|     name|  surname|   sex| age|     location|height|\n",
      "+---+---------+---------+------+----+-------------+------+\n",
      "|  1|    Debra|   Walker|Female|22.0|      Alabama|  1.67|\n",
      "|  2|    Helen|  Jimenez|Female|19.0| Pennsylvania|   1.6|\n",
      "|  3|   Andrew|   Miller|  Male|66.0|     Nebraska|  1.79|\n",
      "|  4|  Kenneth|   Thomas|  Male|35.0|     New York|  1.68|\n",
      "|  5|     John|   Miller|  Male|69.0|   New Mexico|  1.61|\n",
      "|  6| Benjamin|    Lewis|  Male|61.0|     Michigan|  1.93|\n",
      "|  7|    Linda|    Baker|Female|45.0|     New York|  1.74|\n",
      "|  8|  Carolyn|    Lewis|Female|31.0|      Vermont|  1.61|\n",
      "|  9|     Gary|     Wood|  Male|70.0|      Wyoming|  1.83|\n",
      "| 10| Jennifer|   Harris|Female|30.0|        Maine|  1.74|\n",
      "| 11|  Michael|Gutierrez|  Male|31.0|West Virginia|  1.53|\n",
      "| 12|    Linda|    Patel|Female|44.0|      Arizona|  1.51|\n",
      "| 13|  Jessica|  Jimenez|Female|59.0| North Dakota|  1.78|\n",
      "| 14|  Jeffrey|   Martin|  Male|18.0|    Tennessee|  1.61|\n",
      "| 15|  Barbara|    Ortiz|Female|39.0|New Hampshire|  1.55|\n",
      "| 16|    Donna|    Reyes|Female|39.0|  Mississippi|  1.56|\n",
      "| 17|    Kevin|    Green|  Male|34.0|      Arizona|  1.52|\n",
      "| 18|  Kenneth|      Lee|  Male|45.0|     Maryland|  1.74|\n",
      "| 19|    Brian|  Mendoza|  Male|37.0|   California|  1.82|\n",
      "| 20|Alexander| Castillo|  Male|31.0|   New Jersey|  1.54|\n",
      "+---+---------+---------+------+----+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading a CSV with a header:\n",
    "path = \"created_dataset_csv\"\n",
    "df = (spark.read\n",
    "       .option(\"sep\", \",\")\n",
    "       .option(\"header\", True)\n",
    "       .csv(path)\n",
    "        )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- height: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Loading a CSV with schema using inferSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- height: float (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+-------+------+---+------------+------+\n",
      "| id|    name|surname|   sex|age|    location|height|\n",
      "+---+--------+-------+------+---+------------+------+\n",
      "|  1|   Debra| Walker|Female| 22|     Alabama|  1.67|\n",
      "|  2|   Helen|Jimenez|Female| 19|Pennsylvania|   1.6|\n",
      "|  3|  Andrew| Miller|  Male| 66|    Nebraska|  1.79|\n",
      "|  4| Kenneth| Thomas|  Male| 35|    New York|  1.68|\n",
      "|  5|    John| Miller|  Male| 69|  New Mexico|  1.61|\n",
      "|  6|Benjamin|  Lewis|  Male| 61|    Michigan|  1.93|\n",
      "|  7|   Linda|  Baker|Female| 45|    New York|  1.74|\n",
      "|  8| Carolyn|  Lewis|Female| 31|     Vermont|  1.61|\n",
      "|  9|    Gary|   Wood|  Male| 70|     Wyoming|  1.83|\n",
      "| 10|Jennifer| Harris|Female| 30|       Maine|  1.74|\n",
      "+---+--------+-------+------+---+------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading a CSV using inferSchema():\n",
    "\n",
    "path = \"created_dataset_csv\"\n",
    "df2 = (spark.read\n",
    "     .format(\"csv\")\n",
    "     .option(\"sep\", \",\")\n",
    "     .option(\"header\", True)\n",
    "     .option(\"inferSchema\", True)\n",
    "     .load(path)\n",
    "    )\n",
    "\n",
    "# Displaying the schema:\n",
    "print(df_struct.printSchema())\n",
    "\n",
    "# Displaying the DataFrame:\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Load a CSV file with pre-defined schema using DDL string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- height: float (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+-------+------+---+------------+------+\n",
      "| id|    name|surname|   sex|age|    location|height|\n",
      "+---+--------+-------+------+---+------------+------+\n",
      "|  1|   Debra| Walker|Female| 22|     Alabama|  1.67|\n",
      "|  2|   Helen|Jimenez|Female| 19|Pennsylvania|   1.6|\n",
      "|  3|  Andrew| Miller|  Male| 66|    Nebraska|  1.79|\n",
      "|  4| Kenneth| Thomas|  Male| 35|    New York|  1.68|\n",
      "|  5|    John| Miller|  Male| 69|  New Mexico|  1.61|\n",
      "|  6|Benjamin|  Lewis|  Male| 61|    Michigan|  1.93|\n",
      "|  7|   Linda|  Baker|Female| 45|    New York|  1.74|\n",
      "|  8| Carolyn|  Lewis|Female| 31|     Vermont|  1.61|\n",
      "|  9|    Gary|   Wood|  Male| 70|     Wyoming|  1.83|\n",
      "| 10|Jennifer| Harris|Female| 30|       Maine|  1.74|\n",
      "+---+--------+-------+------+---+------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining a DDL schema (SQL-like schema):\n",
    "\n",
    "ddl_schema = \"\"\"\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    surname STRING,\n",
    "    sex STRING,\n",
    "    age INT,\n",
    "    location STRING,\n",
    "    height FLOAT\n",
    "\"\"\"\n",
    "\n",
    "path = \"created_dataset_csv\"\n",
    "\n",
    "df_ddl = (spark.read\n",
    "          .format(\"csv\")\n",
    "          .option(\"header\", True)\n",
    "          .option(\"sep\", \",\")\n",
    "          .schema(ddl_schema)\n",
    "          .load(path)\n",
    "            )\n",
    "\n",
    "print(df_struct.printSchema())\n",
    "df_ddl.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Load a CSV file with predefined schema using StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- height: float (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+-------+------+---+------------+------+\n",
      "| id|    name|surname|   sex|age|    location|height|\n",
      "+---+--------+-------+------+---+------------+------+\n",
      "|  1|   Debra| Walker|Female| 22|     Alabama|  1.67|\n",
      "|  2|   Helen|Jimenez|Female| 19|Pennsylvania|   1.6|\n",
      "|  3|  Andrew| Miller|  Male| 66|    Nebraska|  1.79|\n",
      "|  4| Kenneth| Thomas|  Male| 35|    New York|  1.68|\n",
      "|  5|    John| Miller|  Male| 69|  New Mexico|  1.61|\n",
      "|  6|Benjamin|  Lewis|  Male| 61|    Michigan|  1.93|\n",
      "|  7|   Linda|  Baker|Female| 45|    New York|  1.74|\n",
      "|  8| Carolyn|  Lewis|Female| 31|     Vermont|  1.61|\n",
      "|  9|    Gary|   Wood|  Male| 70|     Wyoming|  1.83|\n",
      "| 10|Jennifer| Harris|Female| 30|       Maine|  1.74|\n",
      "+---+--------+-------+------+---+------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset CSV file with pre-defined StructType:\n",
    "\n",
    "struct_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"surname\", StringType(), True),\n",
    "    StructField(\"sex\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"height\", FloatType(), True),\n",
    "    ])\n",
    "\n",
    "path = \"created_dataset_csv\"\n",
    "\n",
    "df_struct = (spark.read\n",
    "          .format(\"csv\")\n",
    "          .option(\"header\", True)\n",
    "          .option(\"sep\", \",\")\n",
    "          .schema(struct_schema)\n",
    "          .load(path)\n",
    "            )\n",
    "\n",
    "print(df_struct.printSchema())\n",
    "df_struct.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Load a DataFrame from parquet file and select only some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|    name|surname|\n",
      "+--------+-------+\n",
      "|   Debra| Walker|\n",
      "|   Helen|Jimenez|\n",
      "|  Andrew| Miller|\n",
      "| Kenneth| Thomas|\n",
      "|    John| Miller|\n",
      "|Benjamin|  Lewis|\n",
      "|   Linda|  Baker|\n",
      "| Carolyn|  Lewis|\n",
      "|    Gary|   Wood|\n",
      "|Jennifer| Harris|\n",
      "+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.parquet(\"created_dataset.parquet\").select([\"name\",\"surname\"])\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+------+---+------------+------+\n",
      "| id|    name|surname|   sex|age|    location|height|\n",
      "+---+--------+-------+------+---+------------+------+\n",
      "|  1|   Debra| Walker|Female| 22|     Alabama|  1.67|\n",
      "|  2|   Helen|Jimenez|Female| 19|Pennsylvania|   1.6|\n",
      "|  3|  Andrew| Miller|  Male| 66|    Nebraska|  1.79|\n",
      "|  4| Kenneth| Thomas|  Male| 35|    New York|  1.68|\n",
      "|  5|    John| Miller|  Male| 69|  New Mexico|  1.61|\n",
      "|  6|Benjamin|  Lewis|  Male| 61|    Michigan|  1.93|\n",
      "|  7|   Linda|  Baker|Female| 45|    New York|  1.74|\n",
      "|  8| Carolyn|  Lewis|Female| 31|     Vermont|  1.61|\n",
      "|  9|    Gary|   Wood|  Male| 70|     Wyoming|  1.83|\n",
      "| 10|Jennifer| Harris|Female| 30|       Maine|  1.74|\n",
      "+---+--------+-------+------+---+------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the DataFrame before transformations:\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting and dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|surname|age|\n",
      "+---+-------+---+\n",
      "|  1| Walker| 22|\n",
      "|  2|Jimenez| 19|\n",
      "|  3| Miller| 66|\n",
      "|  4| Thomas| 35|\n",
      "|  5| Miller| 69|\n",
      "+---+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting 3 columns from a DataFrame:\n",
    "(df\n",
    " .select([\"id\",\"surname\",\"age\"])\n",
    " .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+---+------+\n",
      "|    name|surname|   sex|age|height|\n",
      "+--------+-------+------+---+------+\n",
      "|   Debra| Walker|Female| 22|  1.67|\n",
      "|   Helen|Jimenez|Female| 19|   1.6|\n",
      "|  Andrew| Miller|  Male| 66|  1.79|\n",
      "| Kenneth| Thomas|  Male| 35|  1.68|\n",
      "|    John| Miller|  Male| 69|  1.61|\n",
      "|Benjamin|  Lewis|  Male| 61|  1.93|\n",
      "|   Linda|  Baker|Female| 45|  1.74|\n",
      "| Carolyn|  Lewis|Female| 31|  1.61|\n",
      "|    Gary|   Wood|  Male| 70|  1.83|\n",
      "|Jennifer| Harris|Female| 30|  1.74|\n",
      "+--------+-------+------+---+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting 5 columns from a DataFrame by dropping 2 columns:\n",
    "(df\n",
    " .drop(\"id\",\"location\")\n",
    " .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+------+---+------------+------+\n",
      "|customer_id|    name|surname|gender|age|       state|height|\n",
      "+-----------+--------+-------+------+---+------------+------+\n",
      "|          1|   Debra| Walker|Female| 22|     Alabama|  1.67|\n",
      "|          2|   Helen|Jimenez|Female| 19|Pennsylvania|   1.6|\n",
      "|          3|  Andrew| Miller|  Male| 66|    Nebraska|  1.79|\n",
      "|          4| Kenneth| Thomas|  Male| 35|    New York|  1.68|\n",
      "|          5|    John| Miller|  Male| 69|  New Mexico|  1.61|\n",
      "|          6|Benjamin|  Lewis|  Male| 61|    Michigan|  1.93|\n",
      "|          7|   Linda|  Baker|Female| 45|    New York|  1.74|\n",
      "|          8| Carolyn|  Lewis|Female| 31|     Vermont|  1.61|\n",
      "|          9|    Gary|   Wood|  Male| 70|     Wyoming|  1.83|\n",
      "|         10|Jennifer| Harris|Female| 30|       Maine|  1.74|\n",
      "+-----------+--------+-------+------+---+------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df\n",
    " .withColumnRenamed(\"id\", \"customer_id\")\n",
    " .withColumnRenamed(\"sex\", \"gender\")\n",
    " .withColumnRenamed(\"location\", \"state\")\n",
    " .show(10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------------+\n",
      "|customer_id|gender|       state|\n",
      "+-----------+------+------------+\n",
      "|          1|Female|     Alabama|\n",
      "|          2|Female|Pennsylvania|\n",
      "|          3|  Male|    Nebraska|\n",
      "|          4|  Male|    New York|\n",
      "|          5|  Male|  New Mexico|\n",
      "|          6|  Male|    Michigan|\n",
      "|          7|Female|    New York|\n",
      "|          8|Female|     Vermont|\n",
      "|          9|  Male|     Wyoming|\n",
      "|         10|Female|       Maine|\n",
      "+-----------+------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming columns by \"select\" + \"alias\":\n",
    "(df\n",
    " .select(\n",
    "     col(\"id\").alias(\"customer_id\"),\n",
    "     col(\"sex\").alias(\"gender\"),\n",
    "     col(\"location\").alias(\"state\")\n",
    "     )\n",
    " .show(10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting column content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+------+---+------------+------+--------------+---------------+----------------+\n",
      "| id|    name|surname|   sex|age|    location|height|height (parts)|height (m part)|height (cm part)|\n",
      "+---+--------+-------+------+---+------------+------+--------------+---------------+----------------+\n",
      "|  1|   Debra| Walker|Female| 22|     Alabama|  1.67|       [1, 67]|              1|              67|\n",
      "|  2|   Helen|Jimenez|Female| 19|Pennsylvania|   1.6|        [1, 6]|              1|               6|\n",
      "|  3|  Andrew| Miller|  Male| 66|    Nebraska|  1.79|       [1, 79]|              1|              79|\n",
      "|  4| Kenneth| Thomas|  Male| 35|    New York|  1.68|       [1, 68]|              1|              68|\n",
      "|  5|    John| Miller|  Male| 69|  New Mexico|  1.61|       [1, 61]|              1|              61|\n",
      "|  6|Benjamin|  Lewis|  Male| 61|    Michigan|  1.93|       [1, 93]|              1|              93|\n",
      "|  7|   Linda|  Baker|Female| 45|    New York|  1.74|       [1, 74]|              1|              74|\n",
      "|  8| Carolyn|  Lewis|Female| 31|     Vermont|  1.61|       [1, 61]|              1|              61|\n",
      "|  9|    Gary|   Wood|  Male| 70|     Wyoming|  1.83|       [1, 83]|              1|              83|\n",
      "| 10|Jennifer| Harris|Female| 30|       Maine|  1.74|       [1, 74]|              1|              74|\n",
      "+---+--------+-------+------+---+------------+------+--------------+---------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, length\n",
    "\n",
    "(df\n",
    " .withColumn(\"height (parts)\", split(col(\"height\"), \"\\.\"))  # Dot is in regex as \"\\.\"\n",
    " .withColumn(\"height (m part)\", split(col(\"height\"), \"\\.\").getItem(0))  # Using .getItem()\n",
    " .withColumn(\"height (cm part)\", split(col(\"height\"), \"\\.\")[1])         # Using [x]\n",
    " .show(10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------+-------------------------------------------------+\n",
      "|name     |count|average_height|states                                           |\n",
      "+---------+-----+--------------+-------------------------------------------------+\n",
      "|Jack     |4    |1.58          |[Colorado, Massachusetts, New Hampshire, Wyoming]|\n",
      "|Alexander|4    |1.7           |[Hawaii, Indiana, Michigan, New Jersey]          |\n",
      "|Benjamin |3    |1.78          |[Kentucky, Michigan, Virginia]                   |\n",
      "|Gary     |3    |1.76          |[Kentucky, Pennsylvania, Wyoming]                |\n",
      "|Michelle |3    |1.87          |[Georgia, Nebraska, Tennessee]                   |\n",
      "|Kevin    |3    |1.62          |[Arizona, Kentucky, New York]                    |\n",
      "|Charles  |3    |1.78          |[Florida, Idaho, Montana]                        |\n",
      "+---------+-----+--------------+-------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- average_height: double (nullable = true)\n",
      " |-- states: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering names with occurences equal to 3 or more:\n",
    "\n",
    "from pyspark.sql.functions import count, avg, round, collect_list, sort_array, array_contains\n",
    "\n",
    "df_filtered = (df\n",
    " .groupBy(\"name\")\n",
    " .agg(\n",
    "     count(\"name\").alias(\"count\"),\n",
    "     round(avg(\"height\"), 2).alias(\"average_height\"),\n",
    "     collect_list(\"location\").alias(\"states\")\n",
    "     )\n",
    " .filter((col(\"count\") >= 3) & (col(\"count\") <= 4)) # Choosing name occurences >= 3 and <= 4\n",
    " .sort(col(\"count\").desc())  # Sorting the list by descending occurences of names\n",
    " .withColumn(\"states\", sort_array(\"states\", asc=True))  # Sorting a \"states\" array\n",
    "    )\n",
    "\n",
    "df_filtered.show(truncate=False)  # \"Truncate=False\" to show all the locations\n",
    "df_filtered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- average_height: double (nullable = true)\n",
      " |-- states: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+---------+-----+--------------+-------------------------------------------------+\n",
      "|name     |count|average_height|states                                           |\n",
      "+---------+-----+--------------+-------------------------------------------------+\n",
      "|Alexander|4    |1.7           |[Hawaii, Indiana, Michigan, New Jersey]          |\n",
      "|Jack     |4    |1.58          |[Colorado, Massachusetts, New Hampshire, Wyoming]|\n",
      "|Benjamin |3    |1.78          |[Kentucky, Michigan, Virginia]                   |\n",
      "|Gary     |3    |1.76          |[Kentucky, Pennsylvania, Wyoming]                |\n",
      "|Kevin    |3    |1.62          |[Arizona, Kentucky, New York]                    |\n",
      "|Michelle |3    |1.87          |[Georgia, Nebraska, Tennessee]                   |\n",
      "|Charles  |3    |1.78          |[Florida, Idaho, Montana]                        |\n",
      "+---------+-----+--------------+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Changing a schema of a DataFrame to Nullable:\n",
    "\n",
    "new_schema = StructType([\n",
    "    StructField(\"name\", StringType(), nullable=True),\n",
    "    StructField(\"count\", LongType(), nullable=True),\n",
    "    StructField(\"average_height\", DoubleType(), nullable=True),\n",
    "    StructField(\"states\", ArrayType(StringType()), nullable=True),\n",
    "    ])\n",
    "\n",
    "df_filtered = spark.createDataFrame(df_filtered.rdd, schema=new_schema)\n",
    "df_filtered.printSchema()\n",
    "df_filtered.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------+----+-------------+------+\n",
      "| id|     name| surname|   sex| age|     location|height|\n",
      "+---+---------+--------+------+----+-------------+------+\n",
      "| 56|Alexander|   White|  Male|28.0|      Indiana|  1.54|\n",
      "| 20|Alexander|Castillo|  Male|31.0|   New Jersey|  1.54|\n",
      "| 36|Alexander|  Torres|  Male|55.0|     Michigan|   2.0|\n",
      "| 89|Alexander|   Scott|  Male|66.0|       Hawaii|  1.73|\n",
      "| 39|     Jack|  Miller|  Male|32.0|New Hampshire|  1.57|\n",
      "| 25|     Jack|  Watson|  Male|38.0|      Wyoming|  1.52|\n",
      "| 31|     Jack|   Smith|  Male|56.0|Massachusetts|  1.57|\n",
      "| 49|     Jack|Mitchell|  Male|66.0|     Colorado|  1.66|\n",
      "| 10| Jennifer|  Harris|Female|30.0|        Maine|  1.74|\n",
      "|  5|     John|  Miller|  Male|69.0|   New Mexico|  1.61|\n",
      "+---+---------+--------+------+----+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# String match searching:\n",
    "\n",
    "(df\n",
    " .filter(col(\"name\").isin(\"John\", \"Jennifer\", \"Alexander\", \"Jack\"))\n",
    " .sort(\"name\", \"age\")\n",
    " .show()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding an array and filtering it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------+-------------+\n",
      "|name     |count|average_height|states       |\n",
      "+---------+-----+--------------+-------------+\n",
      "|Alexander|4    |1.7           |Indiana      |\n",
      "|Alexander|4    |1.7           |New Jersey   |\n",
      "|Jack     |4    |1.58          |Wyoming      |\n",
      "|Jack     |4    |1.58          |New Hampshire|\n",
      "|Benjamin |3    |1.78          |Virginia     |\n",
      "|Charles  |3    |1.78          |Montana      |\n",
      "|Gary     |3    |1.76          |Pennsylvania |\n",
      "|Gary     |3    |1.76          |Wyoming      |\n",
      "|Kevin    |3    |1.62          |Arizona      |\n",
      "|Kevin    |3    |1.62          |New York     |\n",
      "|Michelle |3    |1.87          |Georgia      |\n",
      "|Michelle |3    |1.87          |Nebraska     |\n",
      "+---------+-----+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, avg, round, collect_list, sort_array, array_contains, explode\n",
    "\n",
    "(df\n",
    " .groupBy(\"name\")\n",
    " .agg(\n",
    "     count(\"name\").alias(\"count\"),\n",
    "     round(avg(\"height\"), 2).alias(\"average_height\"),\n",
    "     collect_list(\"location\").alias(\"states\")\n",
    "     )\n",
    " .filter(col(\"count\") >= 3) # Choosing name occurences >= 3\n",
    " .select(\"name\", \"count\", \"average_height\", explode(\"states\").alias(\"states\"))\n",
    " .sort(col(\"count\").desc(), \"name\")  # Sorting the list by descending occurences of names\n",
    " .filter(\n",
    "     ( col(\"states\").contains(\"New\") | col(\"states\").endswith(\"a\") | col(\"states\").like(\"%in%\") ) \n",
    "     & ( ~col(\"states\").isin(\"Florida\") )\n",
    "    )  # Filtering the exploded array\n",
    " .show(truncate=False)  # \"Truncate=False\" to show all the locations\n",
    "    )\n",
    "\n",
    "# .filter(col(\"states\").array_contains(\"New\") | col(\"name\").endswith(\"a\") | col(\"states\").like(\"%in%\"))\n",
    "# .withColumn(\"states\", sort_array(\"states\", asc=True))  # Sorting a \"states\" array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------+----------------------------------------+\n",
      "|name     |count|average_height|states                                  |\n",
      "+---------+-----+--------------+----------------------------------------+\n",
      "|Alexander|4    |1.7           |[New Jersey, Michigan, Indiana, Hawaii] |\n",
      "|Jack     |4    |1.58          |[Massachusetts, New Hampshire, Colorado]|\n",
      "|Benjamin |3    |1.78          |[Michigan, Kentucky]                    |\n",
      "|Charles  |3    |1.78          |[Florida, Montana, Idaho]               |\n",
      "|Gary     |3    |1.76          |[Pennsylvania, Kentucky]                |\n",
      "|Kevin    |3    |1.62          |[Arizona, New York, Kentucky]           |\n",
      "|Michelle |3    |1.87          |[Georgia, Tennessee, Nebraska]          |\n",
      "+---------+-----+--------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Directly filtering an array without exploding it:\n",
    "\n",
    "from pyspark.sql.functions import count, avg, round, collect_list, sort_array, array_contains, expr\n",
    "\n",
    "(df\n",
    " .groupBy(\"name\")\n",
    " .agg(\n",
    "     count(\"name\").alias(\"count\"),\n",
    "     round(avg(\"height\"), 2).alias(\"average_height\"),\n",
    "     collect_list(\"location\").alias(\"states\")\n",
    "     )\n",
    " .filter(col(\"count\") >= 3) # Choosing name occurences >= 3\n",
    " .withColumn(\"states\", expr(\"filter(states, state -> NOT state like '%in%')\"))  # Filtering the array\n",
    " .sort(col(\"count\").desc(), \"name\")  # Sorting the list by descending occurences of names\n",
    " .show(truncate=False)  # \"Truncate=False\" to show all the locations\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"height_sqrt\", sqrt(col(\"height\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing column data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"age\").cast(\"float\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using **SQL** to transform DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+---+-------------+\n",
      "| id|     name| surname|age|        state|\n",
      "+---+---------+--------+---+-------------+\n",
      "|  4|  Kenneth|  Thomas| 35|     New York|\n",
      "| 20|Alexander|Castillo| 31|   New Jersey|\n",
      "| 39|     Jack|  Miller| 32|New Hampshire|\n",
      "| 40|    Kevin|    Hall| 23|     New York|\n",
      "| 71|    Susan|Campbell| 20|New Hampshire|\n",
      "+---+---------+--------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a SQL table (as a view):\n",
    "df.createOrReplaceTempView(\"dataset\")\n",
    "\n",
    "# Reading the created SQL table:\n",
    "spark.sql('''\n",
    "\n",
    "  SELECT\n",
    "     id\n",
    "    ,name\n",
    "    ,surname\n",
    "    ,age\n",
    "    ,location as state\n",
    "  FROM dataset\n",
    "  WHERE 1=1\n",
    "    AND (location like \"%New%\") \n",
    "    AND (age between 20 and 35)\n",
    "    \n",
    "    ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+\n",
      "| id|   name|surname|\n",
      "+---+-------+-------+\n",
      "|  1|  Debra| Walker|\n",
      "|  2|  Helen|Jimenez|\n",
      "|  3| Andrew| Miller|\n",
      "|  4|Kenneth| Thomas|\n",
      "|  5|   John| Miller|\n",
      "+---+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+------+----+------+\n",
      "| id|   sex| age|height|\n",
      "+---+------+----+------+\n",
      "|  1|Female|22.0|  1.67|\n",
      "|  2|Female|19.0|   1.6|\n",
      "|  3|  Male|66.0|  1.79|\n",
      "|  4|  Male|35.0|  1.68|\n",
      "|  5|  Male|69.0|  1.61|\n",
      "+---+------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrames for Joining:\n",
    "\n",
    "df_1 = df.select(\"id\",\"name\",\"surname\")\n",
    "df_2 = df.select(\"id\",\"sex\",\"age\",\"height\")\n",
    "\n",
    "df_1.show(5)\n",
    "df_2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+------+----+------+\n",
      "| id|    name|surname|   sex| age|height|\n",
      "+---+--------+-------+------+----+------+\n",
      "|  1|   Debra| Walker|Female|22.0|  1.67|\n",
      "|  2|   Helen|Jimenez|Female|19.0|   1.6|\n",
      "|  3|  Andrew| Miller|  Male|66.0|  1.79|\n",
      "|  4| Kenneth| Thomas|  Male|35.0|  1.68|\n",
      "|  5|    John| Miller|  Male|69.0|  1.61|\n",
      "|  6|Benjamin|  Lewis|  Male|61.0|  1.93|\n",
      "|  7|   Linda|  Baker|Female|45.0|  1.74|\n",
      "|  8| Carolyn|  Lewis|Female|31.0|  1.61|\n",
      "|  9|    Gary|   Wood|  Male|70.0|  1.83|\n",
      "| 10|Jennifer| Harris|Female|30.0|  1.74|\n",
      "+---+--------+-------+------+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inner join:\n",
    "df_1.join(df_2, on=\"id\", how=\"inner\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+---+------+----+------+\n",
      "| id| name|surname| id|   sex| age|height|\n",
      "+---+-----+-------+---+------+----+------+\n",
      "|  1|Debra| Walker|  1|Female|22.0|  1.67|\n",
      "|  1|Debra| Walker|  2|Female|19.0|   1.6|\n",
      "|  1|Debra| Walker|  3|  Male|66.0|  1.79|\n",
      "|  1|Debra| Walker|  4|  Male|35.0|  1.68|\n",
      "|  1|Debra| Walker|  5|  Male|69.0|  1.61|\n",
      "|  1|Debra| Walker|  6|  Male|61.0|  1.93|\n",
      "|  1|Debra| Walker|  7|Female|45.0|  1.74|\n",
      "|  1|Debra| Walker|  8|Female|31.0|  1.61|\n",
      "|  1|Debra| Walker|  9|  Male|70.0|  1.83|\n",
      "|  1|Debra| Walker| 10|Female|30.0|  1.74|\n",
      "+---+-----+-------+---+------+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross join:\n",
    "df_1.crossJoin(df_2).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+---+------+----+------+\n",
      "| id|  name|surname| id|   sex| age|height|\n",
      "+---+------+-------+---+------+----+------+\n",
      "|  1| Debra| Walker|  1|Female|22.0|  1.67|\n",
      "|  2| Helen|Jimenez|  1|Female|22.0|  1.67|\n",
      "|  3|Andrew| Miller|  1|Female|22.0|  1.67|\n",
      "|  1| Debra| Walker|  2|Female|19.0|   1.6|\n",
      "|  2| Helen|Jimenez|  2|Female|19.0|   1.6|\n",
      "|  3|Andrew| Miller|  2|Female|19.0|   1.6|\n",
      "|  1| Debra| Walker|  3|  Male|66.0|  1.79|\n",
      "|  2| Helen|Jimenez|  3|  Male|66.0|  1.79|\n",
      "|  3|Andrew| Miller|  3|  Male|66.0|  1.79|\n",
      "+---+------+-------+---+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# More complex Cross join:\n",
    "((df_1.filter(col(\"id\") <= 3))\n",
    " .crossJoin(df_2.filter(col(\"id\") <= 3))\n",
    " .show(20)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pamti pro driver/executor podle velikosti dat\n",
    "- exekun pln - explain()\n",
    "- sparksession time - start.time, \n",
    "- broadcasting a small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caching the DataFrame to a memory:\n",
    "from pyspark import StorageLevel\n",
    "df.persist(StorageLevel.MEMORY_ONLY).count()   # count() action is used to trigger the caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "print(df.storageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql.shuffle.partitions\n",
    "spark.sql.autoBroadcastJoinThreshold\n",
    "counter = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting a DataFrame in a join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------+\n",
      "| id|name|surname_abbr|\n",
      "+---+----+------------+\n",
      "|  1| Deb|         Wal|\n",
      "|  2| Hel|         Jim|\n",
      "|  3| And|         Mil|\n",
      "|  4| Ken|         Tho|\n",
      "|  5| Joh|         Mil|\n",
      "+---+----+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-------+-------+------+----+------------+------+\n",
      "| id|   name|surname|   sex| age|    location|height|\n",
      "+---+-------+-------+------+----+------------+------+\n",
      "|  1|  Debra| Walker|Female|22.0|     Alabama|  1.67|\n",
      "|  2|  Helen|Jimenez|Female|19.0|Pennsylvania|   1.6|\n",
      "|  3| Andrew| Miller|  Male|66.0|    Nebraska|  1.79|\n",
      "|  4|Kenneth| Thomas|  Male|35.0|    New York|  1.68|\n",
      "|  5|   John| Miller|  Male|69.0|  New Mexico|  1.61|\n",
      "+---+-------+-------+------+----+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrames for broadcasting:\n",
    "\n",
    "from pyspark.sql.functions import substring, col\n",
    "\n",
    "\n",
    "path = \"created_dataset_csv\"\n",
    "df_large = (spark.read\n",
    "       .option(\"sep\", \",\")\n",
    "       .option(\"header\", True)\n",
    "       .csv(path)\n",
    "        )\n",
    "\n",
    "df_small = (df_large\n",
    " .withColumn(\"name\", substring(col(\"name\"), 0, 3))\n",
    " .withColumn(\"surname_abbr\", substring(col(\"surname\"), 0, 3))\n",
    " .select(\"id\",\"name\",\"surname_abbr\")\n",
    "    )\n",
    "\n",
    "df_small.show(5)\n",
    "df_large.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_large.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartitioning larger DataFrame to more partitions than 1:\n",
    "df_large = df_large.repartition(4)\n",
    "\n",
    "# Displaying the number of partitions:\n",
    "df_large.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+------+----+------------+------+----+------------+\n",
      "| id|   name|surname|   sex| age|    location|height|name|surname_abbr|\n",
      "+---+-------+-------+------+----+------------+------+----+------------+\n",
      "|  1|  Debra| Walker|Female|22.0|     Alabama|  1.67| Deb|         Wal|\n",
      "|  2|  Helen|Jimenez|Female|19.0|Pennsylvania|   1.6| Hel|         Jim|\n",
      "|  3| Andrew| Miller|  Male|66.0|    Nebraska|  1.79| And|         Mil|\n",
      "|  4|Kenneth| Thomas|  Male|35.0|    New York|  1.68| Ken|         Tho|\n",
      "|  5|   John| Miller|  Male|69.0|  New Mexico|  1.61| Joh|         Mil|\n",
      "+---+-------+-------+------+----+------------+------+----+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "Number of partitions: 1\n"
     ]
    }
   ],
   "source": [
    "# Performing a join with broadcasting:\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_b = df_large.join(broadcast(df_small), on=\"id\", how=\"inner\")\n",
    "#df_b = df_large.join(broadcast(df_small), df_large.id==df_small.id, how=\"inner\")\n",
    "\n",
    "print(df_b.show(5))\n",
    "print(\"Number of partitions:\", df.rdd.getNumPartitions())\n",
    "\n",
    "# Deleting cached copies of df_1 DataFrame:\n",
    "# df_1.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|  surname|\n",
      "+---------+\n",
      "|   Walker|\n",
      "|  Jimenez|\n",
      "|   Miller|\n",
      "|   Thomas|\n",
      "|   Miller|\n",
      "|    Lewis|\n",
      "|    Baker|\n",
      "|    Lewis|\n",
      "|     Wood|\n",
      "|   Harris|\n",
      "|Gutierrez|\n",
      "|    Patel|\n",
      "|  Jimenez|\n",
      "|   Martin|\n",
      "|    Ortiz|\n",
      "|    Reyes|\n",
      "|    Green|\n",
      "|      Lee|\n",
      "|  Mendoza|\n",
      "| Castillo|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_b.select(df_b.columns[2:3]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#19036, name#19037, surname#19038, sex#19039, age#19040, location#19041, height#19042, name_abbr#19051, surname_abbr#19062]\n",
      "   +- BroadcastHashJoin [id#19036], [id#19205], Inner, BuildRight, false\n",
      "      :- Exchange RoundRobinPartitioning(4), REPARTITION_BY_NUM, [id=#13806]\n",
      "      :  +- Filter isnotnull(id#19036)\n",
      "      :     +- FileScan csv [id#19036,name#19037,surname#19038,sex#19039,age#19040,location#19041,height#19042] Batched: false, DataFilters: [isnotnull(id#19036)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jakub.cajzl/created_dataset.csv], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:string,name:string,surname:string,sex:string,age:string,location:string,height:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#13809]\n",
      "         +- Project [id#19205, substring(name#19206, 0, 3) AS name_abbr#19051, substring(surname#19207, 0, 3) AS surname_abbr#19062]\n",
      "            +- Filter isnotnull(id#19205)\n",
      "               +- FileScan csv [id#19205,name#19206,surname#19207] Batched: false, DataFilters: [isnotnull(id#19205)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jakub.cajzl/created_dataset.csv], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:string,name:string,surname:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the execution plan:\n",
    "df_large.join(broadcast(df_small), on=\"id\", how=\"inner\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explain() command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange SinglePartition, REPARTITION_BY_NUM, [id=#1684]\n",
      "   +- Project [id#0 AS cutomer_id#1585]\n",
      "      +- FileScan parquet [id#0] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jakub.cajzl/created_dataset.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"id\").alias(\"cutomer_id\")).explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
